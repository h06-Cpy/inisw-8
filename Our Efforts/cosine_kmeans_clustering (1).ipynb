{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "코사인 거리 기반의 kmeans clustering을 위한 코드"
      ],
      "metadata": {
        "id": "ZdvuT83ja-PN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers datasets"
      ],
      "metadata": {
        "id": "yqcgFDUewyFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install umap-learn"
      ],
      "metadata": {
        "id": "NGE8vgBCwyBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pzxf86FJwx9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#이 부분 자기 폴더 경로에 맞게 바꾸기\n",
        "%cd drive/My\\ Drive/Colab\\ Notebooks/BERT/datas"
      ],
      "metadata": {
        "id": "3VxxpztGwx52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, models, util\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "from sentence_transformers.readers import InputExample\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "GtHJGI-yxDSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import models"
      ],
      "metadata": {
        "id": "C55ts9TRw4Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "import numpy  as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "metadata": {
        "id": "8lXIlKKbw5XW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score"
      ],
      "metadata": {
        "id": "SN4bmq-pw5Uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import TruncatedSVD"
      ],
      "metadata": {
        "id": "EoSYgLOuw5SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.mixture import GaussianMixture"
      ],
      "metadata": {
        "id": "0k_QPXnhw5QC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import umap"
      ],
      "metadata": {
        "id": "ew3wAwyaw5Nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#문서 파일 다운로드\n",
        "!gdown \"1-F2Qgk83uyob9Fg2Od4F2ez4IW2Lb4tQ&confirm=t\""
      ],
      "metadata": {
        "id": "rfpYoIAGyO8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#파일 저장 경로로 바꾸기\n",
        "docs = np.load('/content/drive/MyDrive/Colab Notebooks/BERT/datas/newdata_6_4.npy', allow_pickle=True)\n",
        "docs = pd.Series(docs)\n",
        "#각자 20만개씩 쪼개서 돌리기\n",
        "docs = docs[:200000]"
      ],
      "metadata": {
        "id": "ZjTNvv6EwuhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GbUYA-JwecR"
      },
      "outputs": [],
      "source": [
        "#클러스터 그래프가 엄청 이쁘게 나오는 이유는 이것들이 쿼리이기 때문.\n",
        "from numpy.linalg import norm\n",
        "%matplotlib inline\n",
        "\n",
        "def cosine(x, y):\n",
        "  return np.dot(x, y)/(norm(x) * norm(y))\n",
        "\n",
        "\n",
        "def get_scores(X, labels):\n",
        "  CHI = calinski_harabasz_score(X, labels)\n",
        "  DBI = davies_bouldin_score(X, labels)\n",
        "  return (CHI, DBI)\n",
        "\n",
        "model_list = [\"sentence-transformers/all-mpnet-base-v2\", 'sentence-transformers/multi-qa-distilbert-cos-v1', \"sentence-transformers/all-distilroberta-v1\"]\n",
        "#'PCA', 'SVD',\n",
        "DR_list = ['UMAP']\n",
        "# 'dbscan', , 'hdbscan', 'gmm'\n",
        "clustering_list = ['kmeans']\n",
        "\n",
        "for name in model_list:\n",
        "  model_name = name\n",
        "  embedding_model = models.Transformer(model_name)\n",
        "  pooler = models.Pooling(\n",
        "    embedding_model.get_word_embedding_dimension(),\n",
        "    pooling_mode_mean_tokens=True,\n",
        "    pooling_mode_cls_token=True,\n",
        "    pooling_mode_max_tokens=False,\n",
        "  )\n",
        "  model = SentenceTransformer(model_name)\n",
        "  model_name_re = ' '.join(model_name.split('/'))\n",
        "\n",
        "  #자기 폴더 경로에 맞게 바꾸기 -> 나중에 피클 파일 저장되면 이 코드는 빼버리세요\n",
        "  document_embeddings = model.encode(docs)\n",
        "  with open('/content/drive/MyDrive/Colab Notebooks/BERT/datas/6_4_tweets_embedded_by_'+ model_name_re + '.txt', 'wb') as f:\n",
        "    pickle.dump(document_embeddings, f)\n",
        "\n",
        "  #자기 폴더 경로에 맞게 바꾸기\n",
        "  with open('/content/drive/MyDrive/Colab Notebooks/BERT/datas/6_4_tweets_embedded_by_'+ model_name_re + '.txt', 'rb') as f:\n",
        "    document_embeddings = pickle.load(f)\n",
        "\n",
        "  #자기 폴더 경로에 맞게 바꾸기\n",
        "  directory = \"/content/drive/MyDrive/Colab Notebooks/BERT/results/\" +  model_name_re\n",
        "  os.mkdir(directory)\n",
        "\n",
        "  for DR_name in DR_list:\n",
        "    dr_directory = directory + '/' +' DR: ' + DR_name\n",
        "    os.mkdir(dr_directory)\n",
        "\n",
        "    #PCA는 공분산을 이용해서 데이터의 분포와 단위가 다르면 스케일링 할 필요가 있지만 임베딩된 데이터를 살펴보니 그럴 필요가 없어보임\n",
        "    if DR_name == 'PCA':\n",
        "      X = PCA(n_components=2).fit_transform(document_embeddings)\n",
        "\n",
        "    if DR_name == 'SVD':\n",
        "      X = TruncatedSVD(n_components = 2).fit_transform(document_embeddings)\n",
        "\n",
        "    if DR_name == 'UMAP':\n",
        "      X = umap.UMAP().fit_transform(document_embeddings)\n",
        "\n",
        "    for clustering_name in clustering_list:\n",
        "      if DR_name == 'SVD' and clustering_name == 'kmeans':\n",
        "        continue\n",
        "      cluster_directory = dr_directory + '/' + 'cluster: ' + clustering_name\n",
        "      os.mkdir(cluster_directory)\n",
        "\n",
        "      text_folder = cluster_directory + '/' + 'sentences'\n",
        "      image_folder = cluster_directory + '/' + 'images'\n",
        "      os.mkdir(text_folder)\n",
        "      os.mkdir(image_folder)\n",
        "\n",
        "      if clustering_name == 'dbscan':\n",
        "\n",
        "        #이 부분 반복문으로 eps하고 min_samples 변화시켜야 함\n",
        "        dbscan = DBSCAN(eps=0.5, min_samples=2)\n",
        "        dbscan.fit(X)\n",
        "        cluster_assignment = dbscan.labels_\n",
        "        cls_dist=pd.Series(cluster_assignment).value_counts()\n",
        "\n",
        "        #이 부분 일단 클러스터 중심을 뽑을 수가 있어서 거리기반으로 재봤는데 코사인 기반으로도 해봐야 할 듯\n",
        "        distances = scipy.spatial.distance.cdist(dbscan.core_sample_indices_, X)\n",
        "        centers={}\n",
        "\n",
        "        f = open(text_folder + '/' + str(num_clusters) + ' clusters' + '.txt', 'w')\n",
        "\n",
        "        #각 지표별 score 구하기\n",
        "        CHI, DBI = get_scores(X, dbscan.labels_)\n",
        "\n",
        "        f.write(f'davies_bouldin_score: {DBI}, calinski_harabasz_score: {CHI}')\n",
        "        f.write('\\n')\n",
        "        f.write('#####################################')\n",
        "        f.write('\\n')\n",
        "\n",
        "        for j, d in enumerate(distances):\n",
        "          f.write(str(j) + '번째 클러스터')\n",
        "          f.write('-----------------------------------')\n",
        "          f.write('\\n')\n",
        "          ind = np.argsort(d, axis=0)[:20]\n",
        "          for i, idx in enumerate(ind):\n",
        "            f.write(str(i) + '번째로 가까운 문장: ' + docs[idx])\n",
        "            f.write('\\n')\n",
        "          f.write('\\n')\n",
        "        f.close()\n",
        "\n",
        "        labels= dbscan.labels_\n",
        "        fig, ax = plt.subplots(figsize=(12,12))\n",
        "        plt.scatter(X[:,0], X[:,1], c=labels, s=1, cmap='Paired')\n",
        "        plt.title('model: ' + model_name + ', clusters: ' + str(num_clusters) + ', DR: ' + DR_name + ', cluster: ' + clustering_name)\n",
        "        plt.colorbar()\n",
        "        plt.savefig(image_folder + '/' + 'number of cluster: ' + str(num_clusters) + '.png')\n",
        "\n",
        "      else:\n",
        "        for num_clusters in range(5, 16):\n",
        "          #코사인 기반 kmeans\n",
        "          if clustering_name == 'kmeans':\n",
        "            length = np.sqrt((X**2).sum(axis=1))[:,None]\n",
        "            X = X / length\n",
        "\n",
        "            kmeans = KMeans(n_clusters=num_clusters).fit(X)\n",
        "\n",
        "            len_ = np.sqrt(np.square(kmeans.cluster_centers_).sum(axis=1)[:,None])\n",
        "            centers = kmeans.cluster_centers_ / len_\n",
        "            dist = 1 - np.dot(centers, X.T) # K x N matrix of cosine distances\n",
        "\n",
        "            cluster_assignment = kmeans.labels_\n",
        "            cls_dist=pd.Series(kmeans.labels_).value_counts()\n",
        "\n",
        "            clustered_sentences = [[] for _ in range(num_clusters)]\n",
        "\n",
        "            for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "                clustered_sentences[cluster_id].append(docs[sentence_id])\n",
        "\n",
        "            distances = scipy.spatial.distance.cdist(centers, X)\n",
        "\n",
        "            f = open(text_folder + '/' + str(num_clusters) + ' clusters' + '.txt', 'w')\n",
        "\n",
        "            #각 지표별 score 구하기\n",
        "            CHI, DBI = get_scores(X, kmeans.labels_)\n",
        "\n",
        "            f.write(f'davies_bouldin_score: {DBI}, calinski_harabasz_score: {CHI}')\n",
        "            f.write('\\n')\n",
        "            f.write('#####################################')\n",
        "            f.write('\\n')\n",
        "\n",
        "            for j, d in enumerate(distances):\n",
        "              f.write(str(j) + '번째 클러스터')\n",
        "              f.write('-----------------------------------')\n",
        "              f.write('\\n')\n",
        "\n",
        "              ind = np.argsort(d, axis=0)\n",
        "              center = ind[0]\n",
        "\n",
        "              f.write('클러스터 중심 문장: ' + docs[center])\n",
        "              f.write('\\n')\n",
        "\n",
        "              for i, idx in enumerate(ind[1:10]):\n",
        "                f.write(str(i) + '번째로 가까운 문장: ' + docs[idx])\n",
        "                f.write('\\n')\n",
        "\n",
        "              f.write('####################################')\n",
        "              f.write('\\n')\n",
        "\n",
        "              for i, idx in enumerate(ind[-1:-11:-1]):\n",
        "                f.write(str(i) + '번째로 먼 문장: ' + docs[idx])\n",
        "                f.write('\\n')\n",
        "\n",
        "              f.write('####################################')\n",
        "              f.write('\\n')\n",
        "            f.close()\n",
        "\n",
        "            labels= kmeans.labels_\n",
        "            fig, ax = plt.subplots(figsize=(12,12))\n",
        "            plt.scatter(X[:,0], X[:,1], c=labels, s=1, cmap='Paired')\n",
        "            plt.title('model: ' + model_name + ', clusters: ' + str(num_clusters) + ', DR: ' + DR_name + ', cluster: ' + clustering_name)\n",
        "            plt.colorbar()\n",
        "            plt.savefig(image_folder + '/' + 'number of cluster: ' + str(num_clusters) + '.png')\n",
        "\n",
        "          elif clustering_name == 'hdbscan':\n",
        "            hdbscan = hdbscan.HDBSCAN(min_cluster_size=num_clusters)\n",
        "            cluster_assignment = hdbscan.fit_predict(X)\n",
        "\n",
        "            cls_dist=pd.Series(cluster_assignment).value_counts()\n",
        "            clusters = np.unique(cls_dist.index)\n",
        "\n",
        "            clustered_sentences = [[] for _ in range(num_clusters)]\n",
        "\n",
        "            for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "                clustered_sentences[cluster_id].append(docs[sentence_id])\n",
        "\n",
        "             #각 지표별 score 구하기\n",
        "            CHI, DBI = get_scores(X, hdbscan.labels_)\n",
        "\n",
        "            f = open(text_folder + '/' + str(num_clusters) + ' clusters' + '.txt', 'w')\n",
        "\n",
        "            f.write(f'davies_bouldin_score: {DBI}, calinski_harabasz_score: {CHI}')\n",
        "            f.write('\\n')\n",
        "            f.write('#####################################')\n",
        "            f.write('\\n')\n",
        "\n",
        "            for cluster in clusters:\n",
        "              f.write(str(cluster) + '번째 클러스터')\n",
        "              f.write('-----------------------------------')\n",
        "              f.write('\\n')\n",
        "\n",
        "              for idx, sentence in enumerate(clustered_sentences[cluster][:20]):\n",
        "                f.write(str(idx) + '번째 문장: ' + sentence)\n",
        "                f.write('\\n')\n",
        "              f.write('\\n')\n",
        "            f.close()\n",
        "\n",
        "            labels= hdbscan.labels_\n",
        "            fig, ax = plt.subplots(figsize=(12,12))\n",
        "            plt.scatter(X[:,0], X[:,1], c=labels, s=1, cmap='Paired')\n",
        "            plt.title('model: ' + model_name + ', clusters: ' + str(num_clusters) + ', DR: ' + DR_name + ', cluster: ' + clustering_name)\n",
        "            plt.colorbar()\n",
        "            plt.savefig(image_folder + '/' + 'number of cluster: ' + str(num_clusters) + '.png')\n",
        "\n",
        "          elif clustering_name == 'gmm':\n",
        "            gmm = GaussianMixture(n_components=num_clusters)\n",
        "            gmm.fit(X)\n",
        "            cluster_assignment = gmm.predict(X)\n",
        "            cls_dist=pd.Series(cluster_assignment).value_counts()\n",
        "            clusters = np.unique(cls_dist.index)\n",
        "\n",
        "            clustered_sentences = [[] for _ in range(num_clusters)]\n",
        "            sentence_indices = [[] for _ in range(num_clusters)]\n",
        "\n",
        "            number_of_sentences = 10\n",
        "\n",
        "            for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "                clustered_sentences[cluster_id].append(docs[sentence_id])\n",
        "                #일단 number_of_sentences개씩 밑에서 보여주므로 number_of_sentences개 문장을 저장한다.\n",
        "                if len(sentence_indices[cluster_id]) < number_of_sentences:\n",
        "                  sentence_indices[cluster_id].append(sentence_id)\n",
        "\n",
        "            #GMM은 군집 별 중심이 없기 때문에 그냥 클러스터 내의 텍스트만 뽑아내자\n",
        "            f = open(text_folder + '/' + str(num_clusters) + ' clusters' + '.txt', 'w')\n",
        "\n",
        "            #각 지표별 score 구하기\n",
        "            CHI, DBI = get_scores(X, cluster_assignment)\n",
        "\n",
        "            f.write(f'davies_bouldin_score: {DBI}, calinski_harabasz_score: {CHI}')\n",
        "            f.write('\\n')\n",
        "            f.write('#####################################')\n",
        "            f.write('\\n')\n",
        "\n",
        "            for cluster in clusters:\n",
        "              f.write(str(cluster) + '번째 클러스터')\n",
        "              f.write('-----------------------------------')\n",
        "              f.write('\\n')\n",
        "\n",
        "              for idx, sentence in enumerate(clustered_sentences[cluster][:number_of_sentences]):\n",
        "                nth_sentence_idx = sentence_indices[cluster][idx]\n",
        "\n",
        "                f.write(str(idx) + '번째 기준 문장: ' + sentence)\n",
        "                f.write('\\n')\n",
        "\n",
        "                a = []\n",
        "                target = document_embeddings[nth_sentence_idx]\n",
        "\n",
        "                for idx, sentence in enumerate(document_embeddings):\n",
        "                  a.append((cosine(target, sentence), idx))\n",
        "\n",
        "                a.sort(key = lambda x: x[0], reverse = True)\n",
        "\n",
        "                for idx in range(1, 6):\n",
        "                  f.write(str(idx) + '번째로 유사도가 높은 문장' +  ': ' + docs[a[idx][1]])\n",
        "                  f.write('\\n')\n",
        "                  f.write('코사인 유사도: ' + str(a[idx][0]))\n",
        "                  f.write('\\n')\n",
        "\n",
        "                f.write('#####################################')\n",
        "                f.write('\\n')\n",
        "\n",
        "                for idx in range(1, 6):\n",
        "                  f.write(str(idx) + '번째로 유사도가 낮은 문장' +  ': ' + docs[a[-idx][1]])\n",
        "                  f.write('\\n')\n",
        "                  f.write('코사인 유사도: ' + str(a[-idx][0]))\n",
        "                  f.write('\\n')\n",
        "\n",
        "                f.write('@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@')\n",
        "                f.write('\\n')\n",
        "\n",
        "              f.write('\\n')\n",
        "            f.close()\n",
        "\n",
        "            labels= cluster_assignment\n",
        "            fig, ax = plt.subplots(figsize=(12,12))\n",
        "            plt.scatter(X[:,0], X[:,1], c=labels, s=1, cmap='Paired')\n",
        "            plt.title('model: ' + model_name + ', clusters: ' + str(num_clusters) + ', DR: ' + DR_name + ', cluster: ' + clustering_name)\n",
        "            plt.colorbar()\n",
        "            plt.savefig(image_folder + '/' + 'number of cluster: ' + str(num_clusters) + '.png')\n",
        "\n"
      ]
    }
  ]
}